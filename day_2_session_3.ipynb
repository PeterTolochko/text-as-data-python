{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Dictionaries\n",
    "Authors: \"Petro Tolochko & Fabienne Lind \n",
    "Date: November 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "### Required Packages\n",
    "First, install, and import the required packages for text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.24.4)\n",
      "Requirement already satisfied: krippendorff in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Make sure to install the pandas library if you haven't already, using:\n",
    "!pip install pandas numpy krippendorff\n",
    "\n",
    "import pandas as pd\n",
    "import re # regular expressions\n",
    "import numpy as np\n",
    "import krippendorff\n",
    "import requests\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "For this tasks, we will work with selected headlines from news articles about migration. The data set is a subset of the [REMINDER media corpus](https://doi.org/10.11587/IEGQ1B).\n",
    "\n",
    "Let's load the data first and take a look. Each row represents one news article.\n",
    "\n",
    "For our exercise, we work again with the English headlines (published in UK newspapers). \n",
    "Now, we load the data directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'raw.githubusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 500\n",
      "   Unnamed: 0  id country publication_date           source source_type  \\\n",
      "0           1   1      UK       2013-02-09     Daily Mirror       Print   \n",
      "1           4   4      UK       2012-03-16  telegraph.co.uk      Online   \n",
      "2           5   5      UK       2012-08-27  telegraph.co.uk      Online   \n",
      "3           8   8      UK       2016-12-13     mirror.co.uk      Online   \n",
      "4          11  11      UK       2016-03-03     The Guardian       Print   \n",
      "\n",
      "                                            headline  \\\n",
      "0                 Asylum girl 'fed up' in UK;\\nCOURT   \n",
      "1  Archbishop of Canterbury, Dr Rowan Williams: C...   \n",
      "2  France's 'scandalous' expulsion of Roma camps ...   \n",
      "3  Labour's stance on EU immigration is not susta...   \n",
      "4  'It was petrifying': lorry driver attacked nea...   \n",
      "\n",
      "                                         headline_mt  m_fr_eco  m_fr_lab  \\\n",
      "0                 Asylum girl 'fed up' in UK;\\nCOURT         0         0   \n",
      "1  Archbishop of Canterbury, Dr Rowan Williams: C...         0         0   \n",
      "2  France's 'scandalous' expulsion of Roma camps ...         0         1   \n",
      "3  Labour's stance on EU immigration is not susta...         0         1   \n",
      "4  'It was petrifying': lorry driver attacked nea...         0         1   \n",
      "\n",
      "   m_fr_wel  m_fr_sec  \n",
      "0         0         1  \n",
      "1         0         0  \n",
      "2         1         0  \n",
      "3         0         0  \n",
      "4         1         1  \n",
      "Index(['Unnamed: 0', 'id', 'country', 'publication_date', 'source',\n",
      "       'source_type', 'headline', 'headline_mt', 'm_fr_eco', 'm_fr_lab',\n",
      "       'm_fr_wel', 'm_fr_sec'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file from the URL\n",
    "# articles_en = pd.read_csv(\"https://raw.githubusercontent.com/fabiennelind/text-as-data-in-R/main/data/articles_en.csv\")\n",
    "\n",
    "# Fetch the CSV content\n",
    "url = \"https://raw.githubusercontent.com/fabiennelind/text-as-data-in-R/main/data/articles_en.csv\"\n",
    "response = requests.get(url, verify=False)  # Disable SSL verification\n",
    "\n",
    "# Load into a pandas DataFrame\n",
    "articles_en = pd.read_csv(StringIO(response.text))\n",
    "\n",
    "\n",
    "# Check corpus size\n",
    "corpus_size = len(articles_en)\n",
    "print(f'Corpus size: {corpus_size}')\n",
    "\n",
    "# Display the dataset\n",
    "print(articles_en.head())\n",
    "\n",
    "# Display column names\n",
    "print(articles_en.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Classification with a Dictionary\n",
    "\n",
    "For this tutorial, we like to identify all articles that mention political actors in their headlines. The salience of 'Political actors' is the concept that we like to measure with an automated text analysis method, a dictionary. As a first step, we define the concept more closely.\n",
    "\n",
    "### Concept Definition\n",
    "\n",
    "**Political actors** are here defined as political parties represented in the House of Commons between 2000 and 2017, which is the period in which the articles in our sample where published. Next to these parties, we define UK politicians with a leading role as political actors. To keep the task manageable for this exercise, we focus only on actors highly relevant between 2000 and 2017. \n",
    "\n",
    "We intend to measure the salience of political actors as simple binary variable:\n",
    "1 = At least one political actor is mentioned\n",
    "0 = No political actor is mentioned.\n",
    "\n",
    "### Dictionary creation\n",
    "\n",
    "A dictionary is a set of keywords or phrases that represent the concept of interest. \n",
    "\n",
    "We now start to collect relevant keywords for the dictionary. We start with a list of keywords that we consider most relevant. An example for a relevant keyword is \"Boris Johnson\".\n",
    "For clarity, we here work with two keyword sets: we collect the keywords related to politicians in one vector (here named `politicians`), and keywords related to political parties in another vector (here named `parties`). \n",
    "\n",
    "The keywords are written as regular expressions. A ‘regular expression’ is a pattern that describes a string. To test regular expressions quickly, visit https://regex101.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of politicians\n",
    "politicians = [\n",
    "    \"tony blair\", \n",
    "    \"gordon brown\", \n",
    "    \"david cameron\", \n",
    "    \"theresa may\", \n",
    "    \"boris johnson\", \n",
    "    \"prime minister\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of parties with a regular expression\n",
    "parties = [\n",
    "    \"conservative party\", \n",
    "    \"tor(y|ies)\",  \n",
    "    \"ukip\", \n",
    "    \"labour party\", \n",
    "    \"liberal democrats\", \n",
    "    \"scottish national party\", \n",
    "    \"green party\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some questions:\n",
    "\n",
    "Alternative ways to store the keywords?\n",
    "\n",
    "What other keywords are relevant to measure the concept?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we search the keyword in the headlines, we apply some pre-processing steps to the headlines. For this exercise, we designed the keywords all in lower case, so the headlines have to be lower case too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                   asylum girl 'fed up' in uk;\\ncourt\n",
      "1    archbishop of canterbury, dr rowan williams: c...\n",
      "2    france's 'scandalous' expulsion of roma camps ...\n",
      "3    labour's stance on eu immigration is not susta...\n",
      "4    'it was petrifying': lorry driver attacked nea...\n",
      "Name: headline, dtype: object\n"
     ]
    }
   ],
   "source": [
    "articles_en['headline'] = articles_en['headline'].str.lower() # Convert the 'headline' column to lowercase\n",
    "\n",
    "print(articles_en['headline'].head()) # Display the first few values of the 'headline' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now search the keywords in the article headlines. The re.findall() function finds all occurrences of a keyword in the text. The function can search for regular expression. We here ask to count a pattern in the column `headline` of the dataframe `articles_en`. \n",
    "\n",
    "The patterns to count are the politician keywords and the party keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politicians_count\n",
      "0    473\n",
      "1     23\n",
      "2      4\n",
      "Name: count, dtype: int64\n",
      "parties_count\n",
      "0    478\n",
      "1     15\n",
      "2      6\n",
      "3      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Function to count keywords in a text\n",
    "def count_keywords(text, keywords):\n",
    "    # Count occurrences of each keyword (case-insensitive)\n",
    "    keyword_counts = [len(re.findall(rf\"(?i)\\b{keyword}\\b\", text)) for keyword in keywords]\n",
    "    return sum(keyword_counts)\n",
    "\n",
    "# Add columns for counts of politicians and parties\n",
    "articles_en['politicians_count'] = articles_en['headline'].apply(lambda x: count_keywords(x, politicians))\n",
    "articles_en['parties_count'] = articles_en['headline'].apply(lambda x: count_keywords(x, parties))\n",
    "\n",
    "# Display frequency tables for politicians_count and parties_count\n",
    "print(articles_en['politicians_count'].value_counts())\n",
    "print(articles_en['parties_count'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which keywords were found for each group for each row and create a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politicians_keywords_found\n",
      "                                 473\n",
      "theresa may                       12\n",
      "prime minister                     5\n",
      "david cameron                      5\n",
      "david cameron, prime minister      3\n",
      "boris johnson                      2\n",
      "Name: count, dtype: int64\n",
      "parties_keywords_found\n",
      "                478\n",
      "ukip             11\n",
      "tor(y|ies)       10\n",
      "labour party      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Function to find and list keywords in text\n",
    "def check_keywords(text, keywords):\n",
    "    # Find keywords that are present in the text\n",
    "    found_keywords = [keyword for keyword in keywords if re.search(rf\"(?i)\\b{keyword}\\b\", text)]\n",
    "    # Return the found keywords as a comma-separated string\n",
    "    return \", \".join(found_keywords)\n",
    "\n",
    "# Apply the function to find keywords in the 'headline' column\n",
    "articles_en['politicians_keywords_found'] = articles_en['headline'].apply(lambda x: check_keywords(x, politicians))\n",
    "articles_en['parties_keywords_found'] = articles_en['headline'].apply(lambda x: check_keywords(x, parties))\n",
    "\n",
    "# Display frequency tables for found keywords\n",
    "print(articles_en['politicians_keywords_found'].value_counts())\n",
    "print(articles_en['parties_keywords_found'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we obtained a count, that represents how often the keywords were detected per text. Since we initially proposed a simple binary measurement, we now do some recoding. \n",
    "\n",
    "We add a new column to the dataframe called `actors_d`. This column includes a 1 if at least one of all defined keywords creates a hit, and a 0 if no keyword was found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'actors_d' based on conditions\n",
    "articles_en['actors_d'] = np.where(\n",
    "    (articles_en['parties_count'] >= 1) | (articles_en['politicians_count'] >= 1), 1, 0\n",
    ")\n",
    "\n",
    "# Ensure missing values in 'actors_d' are replaced with 0\n",
    "articles_en['actors_d'] = articles_en['actors_d'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our automated measurement, how many articles mention political actors in their headlines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actors_d\n",
      "0    453\n",
      "1     47\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Descriptive overview of the 'actors_d' column\n",
    "print(articles_en['actors_d'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now managed to get an automated measurement for the variable. **But how valid is this measurement?** Does our small set of keywords represent the concept adequately?\n",
    "\n",
    "A common procedure in automated content analysis is to test construct validity. We ask:\n",
    "How close is this automated measurement to a more trusted measurement: Human understanding of text.\n",
    "Let's put this to practice. \n",
    "\n",
    "## Dictionary validation with a human coded baseline\n",
    "\n",
    "To validate the dictionary, we compare the classifications of the dictionary with the classifications of human coders. \n",
    "\n",
    "We create the human coded baseline together. \n",
    "\n",
    "### Intercoder reliability test\n",
    "\n",
    "To ensure the quality of our manual coding, we first perform an intercoder reliability test. For this tutorial, we select a random set of 10 articles. In a real study the number of observations coded by several coders should be higher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0    id country publication_date               source  \\\n",
      "131         375   375      UK       2006-05-24         The Guardian   \n",
      "445        1314  1314      UK       2013-07-13      telegraph.co.uk   \n",
      "408        1225  1225      UK       2005-01-25         The Guardian   \n",
      "439        1294  1294      UK       2016-05-30         Daily Mirror   \n",
      "7            16    16      UK       2017-11-02  The Daily Telegraph   \n",
      "9            23    23      UK       2001-12-01         The Guardian   \n",
      "362        1098  1098      UK       2016-02-19         mirror.co.uk   \n",
      "328        1008  1008      UK       2003-09-30         Daily Mirror   \n",
      "253         788   788      UK       2015-03-06         The Guardian   \n",
      "443        1312  1312      UK       2013-11-28  The Daily Telegraph   \n",
      "\n",
      "    source_type                                           headline  \\\n",
      "131       Print      dublin urged to translate road safety message   \n",
      "445      Online  keith vaz: immigration backlog 'totally unnacc...   \n",
      "408       Print  howard stirs up migrant storm: un and eu conde...   \n",
      "439       Print                          blair: out not the answer   \n",
      "7         Print              stowaways leap from bus into raf base   \n",
      "9         Print                 in brief: 2,745 lose asylum battle   \n",
      "362      Online  david cameron warns eu summit it's suicide to ...   \n",
      "328       Print  life jail for refugee: killed by dad for being...   \n",
      "253       Print  orange lifeboats used to return asylum seekers...   \n",
      "443       Print            boris: some people too stupid to get on   \n",
      "\n",
      "                                           headline_mt  m_fr_eco  m_fr_lab  \\\n",
      "131      Dublin urged to translate road safety message         0         1   \n",
      "445  Keith Vaz: Immigration backlog 'totally unnacc...         0         0   \n",
      "408  Howard stirs up migrant storm: UN and EU conde...         0         0   \n",
      "439                          Blair: Out not the answer         0         0   \n",
      "7                Stowaways leap from bus into RAF base         0         0   \n",
      "9                   In brief: 2,745 lose asylum battle         0         0   \n",
      "362  David Cameron warns EU summit it's suicide to ...         0         0   \n",
      "328  LIFE JAIL FOR REFUGEE: KILLED BY DAD FOR BEING...         0         1   \n",
      "253  Orange lifeboats used to return asylum seekers...         1         0   \n",
      "443            Boris: Some people too stupid to get on         1         0   \n",
      "\n",
      "     m_fr_wel  m_fr_sec  politicians_count  parties_count  \\\n",
      "131         0         1                  0              0   \n",
      "445         0         0                  0              0   \n",
      "408         0         0                  0              0   \n",
      "439         0         0                  0              0   \n",
      "7           0         1                  0              0   \n",
      "9           0         0                  0              0   \n",
      "362         1         0                  2              0   \n",
      "328         0         1                  0              0   \n",
      "253         0         0                  0              0   \n",
      "443         0         0                  0              0   \n",
      "\n",
      "        politicians_keywords_found parties_keywords_found  actors_d  \n",
      "131                                                               0  \n",
      "445                                                               0  \n",
      "408                                                               0  \n",
      "439                                                               0  \n",
      "7                                                                 0  \n",
      "9                                                                 0  \n",
      "362  david cameron, prime minister                                1  \n",
      "328                                                               0  \n",
      "253                                                               0  \n",
      "443                                                               0  \n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "random_state = 57\n",
    "\n",
    "# Sample 10 rows from the DataFrame\n",
    "intercoder_set = articles_en.sample(n=10, random_state=random_state)\n",
    "\n",
    "# Show the sampled DataFrame\n",
    "print(intercoder_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add an empty column called `actors_m`, so that coders can enter the manual codes. We drop all columns that are not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id actors_m                                           headline\n",
      "131   375               dublin urged to translate road safety message\n",
      "445  1314           keith vaz: immigration backlog 'totally unnacc...\n",
      "408  1225           howard stirs up migrant storm: un and eu conde...\n",
      "439  1294                                   blair: out not the answer\n",
      "7      16                       stowaways leap from bus into raf base\n",
      "9      23                          in brief: 2,745 lose asylum battle\n",
      "362  1098           david cameron warns eu summit it's suicide to ...\n",
      "328  1008           life jail for refugee: killed by dad for being...\n",
      "253   788           orange lifeboats used to return asylum seekers...\n",
      "443  1312                     boris: some people too stupid to get on\n"
     ]
    }
   ],
   "source": [
    "# Add a new column 'actors_m' initialized with empty strings\n",
    "intercoder_set['actors_m'] = \"\"\n",
    "\n",
    "# Select specific columns (id, actors_m, headline)\n",
    "intercoder_set = intercoder_set[['id', 'actors_m', 'headline']]\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(intercoder_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create several duplicates of the intercoder reliability set, one for each coder. We create separate files so that coders code individually and do not peek by mistake.\n",
    "To each of these sets we add the coder name in a new column called `coder_name`.\n",
    "For this example, we now need 2 volunteers. Who would like to code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id actors_m                                           headline  \\\n",
      "131   375               dublin urged to translate road safety message   \n",
      "445  1314           keith vaz: immigration backlog 'totally unnacc...   \n",
      "408  1225           howard stirs up migrant storm: un and eu conde...   \n",
      "439  1294                                   blair: out not the answer   \n",
      "7      16                       stowaways leap from bus into raf base   \n",
      "\n",
      "    coder_name  \n",
      "131     Coder1  \n",
      "445     Coder1  \n",
      "408     Coder1  \n",
      "439     Coder1  \n",
      "7       Coder1  \n",
      "       id actors_m                                           headline  \\\n",
      "131   375               dublin urged to translate road safety message   \n",
      "445  1314           keith vaz: immigration backlog 'totally unnacc...   \n",
      "408  1225           howard stirs up migrant storm: un and eu conde...   \n",
      "439  1294                                   blair: out not the answer   \n",
      "7      16                       stowaways leap from bus into raf base   \n",
      "\n",
      "    coder_name  \n",
      "131     Coder2  \n",
      "445     Coder2  \n",
      "408     Coder2  \n",
      "439     Coder2  \n",
      "7       Coder2  \n"
     ]
    }
   ],
   "source": [
    "# For Coder 1\n",
    "intercoder_set_coder1 = intercoder_set.copy()  # Create a copy of the DataFrame\n",
    "intercoder_set_coder1['coder_name'] = \"Coder1\"  # Add the 'coder_name' column\n",
    "\n",
    "# For Coder 2\n",
    "intercoder_set_coder2 = intercoder_set.copy()  # Create a copy of the DataFrame\n",
    "intercoder_set_coder2['coder_name'] = \"Coder2\"  # Add the 'coder_name' column\n",
    "\n",
    "# Display the resulting DataFrames\n",
    "print(intercoder_set_coder1.head())\n",
    "print(intercoder_set_coder2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write and Read Google Sheets\n",
    "\n",
    "We then want to save the data sets in google sheets. Detailed instructions about the conncection of **Python** and **Google Sheets** can be found in  https://google-auth.readthedocs.io/en/master/\n",
    "https://google-auth.readthedocs.io/en/master/user-guide.html\n",
    "https://medium.com/@jb.ranchana/write-and-append-dataframes-to-google-sheets-in-python-f62479460cf0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gspread\n",
      "  Using cached gspread-6.1.4-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting google-auth\n",
      "  Using cached google_auth-2.36.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib>=0.4.1 (from gspread)\n",
      "  Using cached google_auth_oauthlib-1.2.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth)\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib>=0.4.1->gspread)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2024.2.2)\n",
      "Using cached gspread-6.1.4-py3-none-any.whl (57 kB)\n",
      "Using cached google_auth-2.36.0-py2.py3-none-any.whl (209 kB)\n",
      "Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached google_auth_oauthlib-1.2.1-py2.py3-none-any.whl (24 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: pyasn1, oauthlib, cachetools, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, gspread\n",
      "Successfully installed cachetools-5.5.0 google-auth-2.36.0 google-auth-oauthlib-1.2.1 gspread-6.1.4 oauthlib-3.2.2 pyasn1-0.6.1 pyasn1-modules-0.4.1 requests-oauthlib-2.0.0 rsa-4.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting gspread_dataframe\n",
      "  Using cached gspread_dataframe-4.0.0-py2.py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: gspread>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gspread_dataframe) (6.1.4)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gspread_dataframe) (2.0.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gspread_dataframe) (1.14.0)\n",
      "Requirement already satisfied: google-auth>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gspread>=3.0.0->gspread_dataframe) (2.36.0)\n",
      "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gspread>=3.0.0->gspread_dataframe) (1.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas>=0.24.0->gspread_dataframe) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas>=0.24.0->gspread_dataframe) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas>=0.24.0->gspread_dataframe) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas>=0.24.0->gspread_dataframe) (1.24.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from google-auth>=1.12.0->gspread>=3.0.0->gspread_dataframe) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from google-auth>=1.12.0->gspread>=3.0.0->gspread_dataframe) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from google-auth>=1.12.0->gspread>=3.0.0->gspread_dataframe) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (2.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread>=3.0.0->gspread_dataframe) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (3.2.2)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (2024.2.2)\n",
      "Using cached gspread_dataframe-4.0.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Installing collected packages: gspread_dataframe\n",
      "Successfully installed gspread_dataframe-4.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting google\n",
      "  Using cached google-3.0.0-py2.py3-none-any.whl.metadata (627 bytes)\n",
      "Collecting beautifulsoup4 (from google)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->google)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached google-3.0.0-py2.py3-none-any.whl (45 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, google\n",
      "Successfully installed beautifulsoup4-4.12.3 google-3.0.0 soupsieve-2.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting pydrive\n",
      "  Using cached PyDrive-1.3.1.tar.gz (987 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-api-python-client>=1.2 (from pydrive)\n",
      "  Using cached google_api_python_client-2.154.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting oauth2client>=4.0.0 (from pydrive)\n",
      "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting PyYAML>=3.0 (from pydrive)\n",
      "  Downloading PyYAML-6.0.2-cp38-cp38-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client>=1.2->pydrive)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from google-api-python-client>=1.2->pydrive) (2.36.0)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client>=1.2->pydrive)\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client>=1.2->pydrive)\n",
      "  Using cached google_api_core-2.23.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client>=1.2->pydrive)\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from oauth2client>=4.0.0->pydrive) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from oauth2client>=4.0.0->pydrive) (0.4.1)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from oauth2client>=4.0.0->pydrive) (4.9)\n",
      "Requirement already satisfied: six>=1.6.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from oauth2client>=4.0.0->pydrive) (1.14.0)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->pydrive)\n",
      "  Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->pydrive)\n",
      "  Using cached protobuf-5.29.0-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->pydrive)\n",
      "  Using cached proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.2->pydrive) (5.5.0)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.dev0,>=0.19.0->google-api-python-client>=1.2->pydrive)\n",
      "  Downloading pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2024.2.2)\n",
      "Using cached google_api_python_client-2.154.0-py2.py3-none-any.whl (12.6 MB)\n",
      "Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Downloading PyYAML-6.0.2-cp38-cp38-macosx_10_9_x86_64.whl (183 kB)\n",
      "Using cached google_api_core-2.23.0-py3-none-any.whl (156 kB)\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Using cached proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "Using cached protobuf-5.29.0-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Building wheels for collected packages: pydrive\n",
      "  Building wheel for pydrive (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pydrive: filename=PyDrive-1.3.1-py3-none-any.whl size=27434 sha256=9982b0b5e6845dd412e78e1f7ab5265458a0c75f1a463e0ea06772581dd83230\n",
      "  Stored in directory: /Users/fabiennelind/Library/Caches/pip/wheels/c6/14/12/ccdcc5d3b41661f360f9c7d9f7ea9d1879a5f85aa4ecc8cc6f\n",
      "Successfully built pydrive\n",
      "Installing collected packages: uritemplate, PyYAML, pyparsing, protobuf, proto-plus, httplib2, googleapis-common-protos, oauth2client, google-auth-httplib2, google-api-core, google-api-python-client, pydrive\n",
      "Successfully installed PyYAML-6.0.2 google-api-core-2.23.0 google-api-python-client-2.154.0 google-auth-httplib2-0.2.0 googleapis-common-protos-1.66.0 httplib2-0.22.0 oauth2client-4.1.3 proto-plus-1.25.0 protobuf-5.29.0 pydrive-1.3.1 pyparsing-3.1.4 uritemplate-4.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gspread google-auth\n",
    "!pip install gspread_dataframe\n",
    "!pip install google\n",
    "!pip install pydrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (2.4.1)\n",
      "Requirement already satisfied: absl-py~=0.10 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.2)\n",
      "Collecting numpy~=1.19.2 (from tensorflow)\n",
      "  Downloading numpy-1.19.5-cp37-cp37m-macosx_10_9_x86_64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (4.24.4)\n",
      "Requirement already satisfied: six~=1.15.0 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: wheel~=0.35 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: gast==0.3.3 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.4.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.32.0)\n",
      "Collecting google-auth<2,>=1.6.3 (from tensorboard~=2.4->tensorflow)\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (46.0.0.post20200309)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/fabiennelind/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (2.2.0)\n",
      "Downloading numpy-1.19.5-cp37-cp37m-macosx_10_9_x86_64.whl (15.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.6/15.6 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: numpy, google-auth\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.6\n",
      "    Uninstalling numpy-1.21.6:\n",
      "      Successfully uninstalled numpy-1.21.6\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.36.0\n",
      "    Uninstalling google-auth-2.36.0:\n",
      "      Successfully uninstalled google-auth-2.36.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bertopic 0.13.0 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\n",
      "google-api-core 2.23.0 requires google-auth<3.0.dev0,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "hdbscan 0.8.29 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
      "numba 0.56.4 requires llvmlite<0.40,>=0.39.0dev0, but you have llvmlite 0.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-auth-1.35.0 numpy-1.19.5\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
    "from google.oauth2.service_account import Credentials\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "# Define the scope of the API access\n",
    "scopes = ['https://www.googleapis.com/auth/spreadsheets',\n",
    "          'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "SERVICE_ACCOUNT_FILE = '/Users/fabiennelind/ucloud/Research/APIs/gsheets_creds.json'\n",
    "\n",
    "# Authenticate using service account\n",
    "creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=scopes)\n",
    "\n",
    "# Connect to Google Sheets\n",
    "gc = gspread.authorize(creds)\n",
    "\n",
    "gauth = GoogleAuth()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now save the datasets for the intercoder reliability test as Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different methods to open a google sheet\n",
    "# open a google sheet from its name\n",
    "gs = gc.open('Text as data')\n",
    "\n",
    "# use a key (which can be extracted from the spreadsheet’s id\n",
    "#gs = gc.open_by_key('15ulUYe0zu2aDw9_PQ9f_GFQ42WwTGvmPrk5hWd53Zsk')\n",
    "\n",
    "# paste the entire spreadsheet’s url\n",
    "##gs = gc.open_by_url('https://docs.google.com/spreadsheets/d/15ulUYe0zu2aDw9_PQ9f_GFQ42WwTGvmPrk5hWd53Zsk/edit?gid=0#gid=0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a specific worksheet by name from the sheet\n",
    "worksheet1 = gs.worksheet('Sheet1')\n",
    "worksheet2 = gs.worksheet('Sheet2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data for coder 1 to dataframe\n",
    "worksheet1.clear()\n",
    "set_with_dataframe(worksheet=worksheet1, dataframe=intercoder_set_coder1, include_index=False,\n",
    "include_column_header=True, resize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data for coder 2 to dataframe\n",
    "worksheet2.clear()\n",
    "set_with_dataframe(worksheet=worksheet2, dataframe=intercoder_set_coder2, include_index=False,\n",
    "include_column_header=True, resize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready to code? We will post links for the different files. Read the column `headline`. If the headline mentions a political actor insert `1` in the column `actors_m`. Enter a `0` in `actors_m` if the headline does not mention a political actor.\n",
    "\n",
    "After you finished coding, we read all sheets back (now with manual classifications for `actors_m`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all values from the sheet of coder 1\n",
    "intercoder_set_coder1_c = pd.DataFrame(worksheet1.get_all_values())\n",
    "\n",
    "# Use values of the first row as column names\n",
    "headers = intercoder_set_coder1_c.iloc[0].values\n",
    "intercoder_set_coder1_c.columns = headers\n",
    "intercoder_set_coder1_c.drop(index=0, axis=0, inplace=True)\n",
    "\n",
    "intercoder_set_coder1_c\n",
    "\n",
    "# convert relevant column to numeric\n",
    "intercoder_set_coder1_c['actors_m'] = pd.to_numeric(intercoder_set_coder1_c['actors_m'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all values from the sheet of coder 2\n",
    "intercoder_set_coder2_c = pd.DataFrame(worksheet2.get_all_values())\n",
    "\n",
    "# Use values of the first row as column names\n",
    "headers = intercoder_set_coder2_c.iloc[0].values\n",
    "intercoder_set_coder2_c.columns = headers\n",
    "intercoder_set_coder2_c.drop(index=0, axis=0, inplace=True)\n",
    "\n",
    "intercoder_set_coder2_c\n",
    "\n",
    "\n",
    "# convert relevant column to numeric\n",
    "intercoder_set_coder2_c['actors_m'] = pd.to_numeric(intercoder_set_coder2_c['actors_m'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too calculate the agreement between coders, we first restructure the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>actors_m_Coder1</th>\n",
       "      <th>actors_m_Coder2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1294</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1098</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>788</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1312</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  actors_m_Coder1  actors_m_Coder2\n",
       "0   375              NaN              NaN\n",
       "1  1314              NaN              NaN\n",
       "2  1225              NaN              NaN\n",
       "3  1294              NaN              NaN\n",
       "4    16              NaN              NaN\n",
       "5    23              NaN              NaN\n",
       "6  1098              NaN              NaN\n",
       "7  1008              NaN              NaN\n",
       "8   788              NaN              NaN\n",
       "9  1312              NaN              NaN"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the two dataframes on 'id' to align the codings for each coder\n",
    "\n",
    "merged_df_actors = pd.merge(intercoder_set_coder1_c[['id', 'actors_m']],\n",
    "                            intercoder_set_coder2_c[['id', 'actors_m']],\n",
    "                            on='id',\n",
    "                            suffixes=('_Coder1', '_Coder2')\n",
    "                        )\n",
    "merged_df_actors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 0, 1, 1, 1, 1, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a matrix where rows are items and columns are coders' ratings\n",
    "ratings_actors = merged_df_actors[['actors_m_Coder1', 'actors_m_Coder2']].values.T  # Transpose to match input format\n",
    "ratings_actors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate Krippendorff's alpha for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff's alpha for 'actors_m': 0.6274509803921569\n"
     ]
    }
   ],
   "source": [
    "# Calculate Krippendorff's alpha for nominal data\n",
    "alpha_actors = krippendorff.alpha(reliability_data=ratings_actors, level_of_measurement='nominal')\n",
    "\n",
    "print(f\"Krippendorff's alpha for 'actors_m': {alpha_actors}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If alpha is large enough, we consider the quality of our manual coding as sufficient. We can then start with the creation of a larger manual baseline to be compared with the dictionary classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a manually coded baseline\n",
    "\n",
    "We pick 100 headlines randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "random_state = 576\n",
    "\n",
    "# Sample 10 rows from the DataFrame\n",
    "manual_set = articles_en.sample(n=100, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add again an empty column called `actors_m`, for coders to enter the manual codes. This time, we also add an empty column for the coder names. We split the work. Each of us gets some headlines to code (in a real application: each of the coders would need to take part in the intercoder test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'actors_m' initialized with empty strings\n",
    "manual_set['actors_m'] = \"\"\n",
    "manual_set['coder_name'] = \"\"\n",
    "\n",
    "# Select specific columns (id, actors_m, headline)\n",
    "manual_set = manual_set[['id', 'actors_m', 'headline', 'coder_name']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a google sheet for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a specific worksheet by name from the sheet\n",
    "worksheet3 = gs.worksheet('Sheet3')\n",
    "\n",
    "# write data to sheet\n",
    "worksheet3.clear()\n",
    "set_with_dataframe(worksheet=worksheet3, dataframe=manual_set, include_index=False,\n",
    "include_column_header=True, resize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please open the sheet in your browser. Enter a coding name (free to pick) in the column `coder_name` for a couple of rows first. Then start to enter 1 (political actor in headline mentioned) or 0 (not mentioned) in the column `actors_m` for the rows with your coding name. Our goal is to finish coding of all headlines.\n",
    "\n",
    "\n",
    "After you finish coding, we read all sheets back (now with manual classifications for `actors_m`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all values from the sheet of coder 2\n",
    "manual_set_c = pd.DataFrame(worksheet3.get_all_values())\n",
    "\n",
    "# Use values of the first row as column names\n",
    "headers = manual_set_c.iloc[0].values\n",
    "manual_set_c.columns = headers\n",
    "manual_set_c.drop(index=0, axis=0, inplace=True)\n",
    "\n",
    "manual_set_c\n",
    "\n",
    "\n",
    "# convert relevant column to numeric\n",
    "manual_set_c['actors_m'] = pd.to_numeric(manual_set_c['actors_m'], errors='coerce')\n",
    "manual_set_c['id'] = pd.to_numeric(manual_set_c['id'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create an object, where the manual and automated classifications are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>actors_m</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>country</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>source</th>\n",
       "      <th>source_type</th>\n",
       "      <th>headline</th>\n",
       "      <th>headline_mt</th>\n",
       "      <th>m_fr_eco</th>\n",
       "      <th>m_fr_lab</th>\n",
       "      <th>m_fr_wel</th>\n",
       "      <th>m_fr_sec</th>\n",
       "      <th>politicians_count</th>\n",
       "      <th>parties_count</th>\n",
       "      <th>politicians_keywords_found</th>\n",
       "      <th>parties_keywords_found</th>\n",
       "      <th>actors_d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>345</td>\n",
       "      <td>0</td>\n",
       "      <td>345</td>\n",
       "      <td>UK</td>\n",
       "      <td>2017-06-26</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Print</td>\n",
       "      <td>theresa may's attacks on human rights laws are...</td>\n",
       "      <td>Theresa May's attacks on human rights laws are...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>theresa may</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>UK</td>\n",
       "      <td>2016-12-13</td>\n",
       "      <td>mirror.co.uk</td>\n",
       "      <td>Online</td>\n",
       "      <td>labour's stance on eu immigration is not susta...</td>\n",
       "      <td>Labour's stance on EU immigration is not susta...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>800</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>UK</td>\n",
       "      <td>2013-07-29</td>\n",
       "      <td>Daily Mirror</td>\n",
       "      <td>Print</td>\n",
       "      <td>ad nausea;\\nvoice of the voice@mirror.co.uk</td>\n",
       "      <td>Ad nausea;\\nVOICE OF THE voice@mirror.co.uk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1430</td>\n",
       "      <td>1</td>\n",
       "      <td>1430</td>\n",
       "      <td>UK</td>\n",
       "      <td>2017-09-30</td>\n",
       "      <td>telegraph.co.uk</td>\n",
       "      <td>Online</td>\n",
       "      <td>racists nearly killed ukip this week, but we l...</td>\n",
       "      <td>Racists nearly killed Ukip this week, but we l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>ukip</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>177</td>\n",
       "      <td>UK</td>\n",
       "      <td>2014-02-11</td>\n",
       "      <td>The Daily Telegraph</td>\n",
       "      <td>Print</td>\n",
       "      <td>salmond 'not honest' about border controls;\\ns...</td>\n",
       "      <td>Salmond 'not honest' about border controls;\\nS...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>981</td>\n",
       "      <td>1</td>\n",
       "      <td>981</td>\n",
       "      <td>UK</td>\n",
       "      <td>2017-10-24</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Print</td>\n",
       "      <td>the guardian view on universities and brexit: ...</td>\n",
       "      <td>The Guardian view on universities and Brexit: ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>371</td>\n",
       "      <td>1</td>\n",
       "      <td>371</td>\n",
       "      <td>UK</td>\n",
       "      <td>2002-12-28</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Print</td>\n",
       "      <td>britain 'takes more refugees than is fair': un...</td>\n",
       "      <td>Britain 'takes more refugees than is fair': UN...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>956</td>\n",
       "      <td>1</td>\n",
       "      <td>956</td>\n",
       "      <td>UK</td>\n",
       "      <td>2007-10-24</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Print</td>\n",
       "      <td>europe: eu moves to bring in skilled foreign w...</td>\n",
       "      <td>Europe: EU moves to bring in skilled foreign w...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>561</td>\n",
       "      <td>1</td>\n",
       "      <td>561</td>\n",
       "      <td>UK</td>\n",
       "      <td>2000-04-20</td>\n",
       "      <td>Daily Mirror</td>\n",
       "      <td>Print</td>\n",
       "      <td>bishop: we're no racists</td>\n",
       "      <td>BISHOP: WE'RE NO RACISTS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>UK</td>\n",
       "      <td>2014-09-06</td>\n",
       "      <td>mirror.co.uk</td>\n",
       "      <td>Online</td>\n",
       "      <td>recap: riot breaks out at immigration centre a...</td>\n",
       "      <td>Recap: Riot breaks out at immigration centre a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  actors_m  Unnamed: 0 country publication_date               source  \\\n",
       "0    345         0         345      UK       2017-06-26         The Guardian   \n",
       "1      8         1           8      UK       2016-12-13         mirror.co.uk   \n",
       "2    800         1         800      UK       2013-07-29         Daily Mirror   \n",
       "3   1430         1        1430      UK       2017-09-30      telegraph.co.uk   \n",
       "4    177         1         177      UK       2014-02-11  The Daily Telegraph   \n",
       "..   ...       ...         ...     ...              ...                  ...   \n",
       "95   981         1         981      UK       2017-10-24         The Guardian   \n",
       "96   371         1         371      UK       2002-12-28         The Guardian   \n",
       "97   956         1         956      UK       2007-10-24         The Guardian   \n",
       "98   561         1         561      UK       2000-04-20         Daily Mirror   \n",
       "99    79         1          79      UK       2014-09-06         mirror.co.uk   \n",
       "\n",
       "   source_type                                           headline  \\\n",
       "0        Print  theresa may's attacks on human rights laws are...   \n",
       "1       Online  labour's stance on eu immigration is not susta...   \n",
       "2        Print        ad nausea;\\nvoice of the voice@mirror.co.uk   \n",
       "3       Online  racists nearly killed ukip this week, but we l...   \n",
       "4        Print  salmond 'not honest' about border controls;\\ns...   \n",
       "..         ...                                                ...   \n",
       "95       Print  the guardian view on universities and brexit: ...   \n",
       "96       Print  britain 'takes more refugees than is fair': un...   \n",
       "97       Print  europe: eu moves to bring in skilled foreign w...   \n",
       "98       Print                           bishop: we're no racists   \n",
       "99      Online  recap: riot breaks out at immigration centre a...   \n",
       "\n",
       "                                          headline_mt  m_fr_eco  m_fr_lab  \\\n",
       "0   Theresa May's attacks on human rights laws are...         0         0   \n",
       "1   Labour's stance on EU immigration is not susta...         0         1   \n",
       "2         Ad nausea;\\nVOICE OF THE voice@mirror.co.uk         0         0   \n",
       "3   Racists nearly killed Ukip this week, but we l...         0         0   \n",
       "4   Salmond 'not honest' about border controls;\\nS...         0         1   \n",
       "..                                                ...       ...       ...   \n",
       "95  The Guardian view on universities and Brexit: ...         0         0   \n",
       "96  Britain 'takes more refugees than is fair': UN...         0         0   \n",
       "97  Europe: EU moves to bring in skilled foreign w...         0         1   \n",
       "98                           BISHOP: WE'RE NO RACISTS         0         0   \n",
       "99  Recap: Riot breaks out at immigration centre a...         0         0   \n",
       "\n",
       "    m_fr_wel  m_fr_sec  politicians_count  parties_count  \\\n",
       "0          0         1                  1              0   \n",
       "1          0         0                  0              0   \n",
       "2          0         1                  0              0   \n",
       "3          0         0                  0              1   \n",
       "4          1         0                  0              0   \n",
       "..       ...       ...                ...            ...   \n",
       "95         1         0                  0              0   \n",
       "96         0         1                  0              0   \n",
       "97         0         0                  0              0   \n",
       "98         0         0                  0              0   \n",
       "99         0         0                  0              0   \n",
       "\n",
       "   politicians_keywords_found parties_keywords_found  actors_d  \n",
       "0                 theresa may                                1  \n",
       "1                                                            0  \n",
       "2                                                            0  \n",
       "3                                               ukip         1  \n",
       "4                                                            0  \n",
       "..                        ...                    ...       ...  \n",
       "95                                                           0  \n",
       "96                                                           0  \n",
       "97                                                           0  \n",
       "98                                                           0  \n",
       "99                                                           0  \n",
       "\n",
       "[100 rows x 18 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select and merge\n",
    "manual_set_c = manual_set_c[['id', 'actors_m']]\n",
    "articles_d_m = pd.merge(manual_set_c, articles_en, on='id')\n",
    "len(articles_d_m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare automated with manual classifications \n",
    "\n",
    "We compare the automated classification (in column `actors_d`) with the manual classifications (in column `actors_m`) we use three metrics: Recall, Precision, and F1.\n",
    "The metrics inform us about the quality of the dictionary. All three metrics range from 0 to 1. \n",
    "We assume that our manual classification identified all relevant articles (here: headlines that mention a political actor).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.76\n",
      "Recall: 0.15\n"
     ]
    }
   ],
   "source": [
    "# Calculate True Positives, False Positives, and False Negatives\n",
    "tp = ((articles_d_m['actors_m'] == 1) & (articles_d_m['actors_d'] == 1)).sum()  # True Positives\n",
    "fp = ((articles_d_m['actors_m'] == 0) & (articles_d_m['actors_d'] == 1)).sum()  # False Positives\n",
    "fn = ((articles_d_m['actors_m'] == 1) & (articles_d_m['actors_d'] == 0)).sum()  # False Negatives\n",
    "\n",
    "# Precision and Recall\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall \n",
    "\n",
    "By inspecting recall we can say how many relevant articles are retrieved by the dictionary.\n",
    "A recall of 1.0 means that our dictionary retrieved all relevant articles. \n",
    "A recall of 0.8 means that our dictionary retrieved 80% of all relevant articles. \n",
    "\n",
    "To obtain recall, we calculate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision \n",
    "\n",
    "By inspecting precision we can say how many retrieved articles are relevant.\n",
    "A precision of 1,0 means that all articles retrieved by the dictionary are relevant. \n",
    "A precision of 0.8 means that 80% of the articles that our dictionary retrieved are relevant articles. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
