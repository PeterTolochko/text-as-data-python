---
title: "stm_scaling_r.Rmd"
output: html_document
date: "2024-12-02"
---

```{r, message=FALSE, results='hide'}

# install.packages("tm")                            
# install.packages("tidyverse")                     
# install.packages("ggthemes")                      
# install.packages("ggrepel")
# install.packages("cowplot")
# install.packages("quanteda")
# install.packages("quanteda.textmodels")



# install.packages(quanteda.textplots)
# install.packages("gtools")
# install.packages("sotu")
# install.packages("stm")
# install.packages(c("Rtsne", "rsvd", "geometry", "purrr"))


```

Note: you only need to install the packages once.

We then need load the packages in our environment:

```{r, message=FALSE, results='hide'}
library(tm)
library(tidyverse)
library(ggthemes)
library(ggrepel)
library(quanteda)
library(quanteda.textmodels)
library(gtools)
library(sotu)
library(stm)
library(purrr)
library(quanteda.textplots)

```



# Topic Modeling

We will be using the `stm` package and `sotu` data.

First, let's load up the data:

```{r}
sotu <- sotu_text %>% as_tibble()
sotu_metadata <- sotu_meta %>% as_tibble()




sotu <- bind_cols(sotu, sotu_metadata)

summary(sotu)

```


Let's inspect a text:

```{r}
sotu$value[1]


```

Also, the timeframe in the dataset is quite large (1790 to 2020). We might want to trim it, since the language might have changed significantly in this time.

```{r}
sotu <- sotu %>% 
  filter(year >= 1901)
```


Ok, we need to do basic preprocessing. We can do that immediately within the infrastructure of the `stm` package, with the `textProcessor` function.

This will take a few seconds...

```{r}
sotu_data <- textProcessor(
  sotu$value,
  metadata = sotu
)
```


Ok, now we have the processed object.

```{r}
sotu_data
```
Next, we need to prepare the documents for topic modeling. This is done with the `prepDocuments` function
```{r}
sotu_data_prepd <- prepDocuments(sotu_data$documents,
                                 sotu_data$vocab,
                                 sotu_data$meta)
```


Now we're ready for some modelling!
This will also take some time.

```{r, results='hide'}
sotu_topic_10 <- stm(documents = sotu_data_prepd$documents,
                     sotu_data_prepd$vocab,
                     K = 10,
                     max.em.its = 75,
                     data = sotu_data_prepd$meta,
                     init.type = "Spectral")
```

When the model is done running, we can inspect the output. Different possible ways to do so:

```{r}
labelTopics(sotu_topic_10)

```

Notice the word `will`, `nation`, `year`, `american`, `america`,  etc. appear in every topic. This is not a great topic model.

First, we might want to remove these terms.


```{r}
sotu_data <- textProcessor(
  sotu$value,
  metadata = sotu,
  customstopwords = c("will",
                      "national",
                      "nation",
                      "year",
                      "america",
                      "american",
                      "government",
                      "govern",
                      "can",
                      "must")
)

sotu_data_prepd <- prepDocuments(sotu_data$documents,
                                 sotu_data$vocab,
                                 sotu_data$meta)

sotu_topic_10 <- stm(documents = sotu_data_prepd$documents,
                     sotu_data_prepd$vocab,
                     K = 15,
                     max.em.its = 75,
                     data = sotu_data_prepd$meta,
                     init.type = "Spectral")
```


A little bit better, but still not amazing. Maybe the problem is in the K? How many topics to select? ***NOBODY KNOWS***.

But there are several ways to get a better idea:

### T-SNE initialization

```{r}
sotu_topic_0 <- stm(documents = sotu_data_prepd$documents,
                     sotu_data_prepd$vocab,
                     K = 0,
                     max.em.its = 75,
                     data = sotu_data_prepd$meta,
                     init.type = "Spectral")
```

WOAH, this method recommends 92 topics!!! But this is a greed method: essentially, the best 'fit' for a topic model is when every document is assigned a separate topic. But kind of defeats the purpose.

# Many Models

Second method is estimating several topic models and then comparing "quality statistics", and figuring out which one is better. Let's estimate K = 20, K = 40 and K = 60.

This takes a very long time! (I already ran it)

```{r, print=FALSE}
# many_models <- manyTopics(documents = sotu_data_prepd$documents,
#                      sotu_data_prepd$vocab,
#                      K = c(20, 40, 60),
#                      max.em.its = 75,
#                      data = sotu_data_prepd$meta,
#                      init.type = "Spectral")

load("many_models.RData")
```


```{r}
sapply(many_models$exclusivity, mean)
sapply(many_models$semcoh, mean)

```
Ok, let's choose the K=60 model.

```{r, print=FALSE}
sotu_topic_60 <- stm(documents = sotu_data_prepd$documents,
                    vocab = sotu_data_prepd$vocab,
                    K = 60,
                     prevalence =~ party + s(year),
                     max.em.its = 75,
                     data = sotu_data_prepd$meta,
                     init.type = "Spectral",
                     verbose=FALSE
)
```

Let's see:

```{r}
labelTopics(sotu_topic_60)
plot(sotu_topic_60)
```


```{r}
cloud(sotu_topic_60, topic=13, scale=c(2,.25))

```


```{r}


sotu_topic_60_effects <- estimateEffect(1:20 ~ party + s(year), sotu_topic_60, meta = sotu_data_prepd$meta, uncertainty = "Global")
summary(sotu_topic_60_effects, topics=3)



plot(sotu_topic_60_effects, covariate = "party", topics = c(1:3), model = sotu_topic_60, method = "pointestimate",
     main = "Effect of Party on Topic Proportion", labeltype = "custom",
     custom.labels = c("Republican", "Democratic")
     )




```


# Content difference

```{r, print=FALSE}
sotu_topic_5 <- stm(documents = sotu_data_prepd$documents,
                    vocab = sotu_data_prepd$vocab,
                    K = 5,
                    prevalence = ~party,
                     content =~ party,
                     max.em.its = 75,
                     data = sotu_data_prepd$meta,
                     init.type = "Spectral",
                     verbose=TRUE
)

```


# Text scaling

From [https://burtmonroe.github.io/TextAsDataCourse/Tutorials/IntroductionToWordfish.nb.html](https://burtmonroe.github.io/TextAsDataCourse/Tutorials/IntroductionToWordfish.nb.html)


```{r}
# Irish budget speeches from 2010

toks_irish <- tokens(data_corpus_irishbudget2010, remove_punct = TRUE)
dfmat_irish <- dfm(toks_irish)
tmod_wf <- textmodel_wordfish(dfmat_irish, dir = c(2, 4))
summary(tmod_wf)



textplot_scale1d(tmod_wf)

textplot_scale1d(tmod_wf, groups = dfmat_irish$party)


textplot_scale1d(tmod_wf, margin = "features", 
                 highlighted = c("government", "global", "children", 
                                 "bank", "economy", "the", "citizenship",
                                 "productivity", "deficit"))

```

Topic models can also do unidimensional scaling!

```{r}
dfmat_irish_stm <- quanteda::convert(dfmat_irish, to = "stm")
names(dfmat_irish_stm)

irish_stmfit <- stm(documents = dfmat_irish_stm$documents, 
                     vocab = dfmat_irish_stm$vocab,
                     K = 2,
                     max.em.its = 75,
                     data = dfmat_irish_stm$meta,
                     init.type = "Spectral"
)


compare.df <- cbind(name=rownames(docvars(dfmat_irish)),wordfish = tmod_wf$theta, stm = irish_stmfit$theta[,2])
compare.df


```
