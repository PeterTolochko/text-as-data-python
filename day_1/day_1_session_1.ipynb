{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68e6e50",
   "metadata": {},
   "source": [
    "# Text Representation\n",
    "This notebook demonstrates concepts in text representation and statistical models for text analysis. It includes examples of creating a Document Term Matrix (DTM), calculating word probabilities, and applying the Multinomial Model to predict text origins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a336de6c",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "### Required Packages\n",
    "First, install and import the required packages for text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99a7def",
   "metadata": {},
   "source": [
    "### Document Term Matrix\n",
    "The `Document Term Matrix` is a bag-of-words representation of text. It is a matrix where:\n",
    "\n",
    "- Rows correspond to documents.\n",
    "- Columns correspond to terms (unique words in the corpus).\n",
    "\n",
    "For example:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 & 1 & ... \\\\\n",
    "0 & 2 & ... \\\\\n",
    "... & ... & ...\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "represents a document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12bac96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "# !pip install nltk numpy pandas scikit-learn\n",
    "\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from math import factorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de38422",
   "metadata": {},
   "source": [
    "## Text Representation\n",
    "### Creating a Sample Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b571a584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 3\n"
     ]
    }
   ],
   "source": [
    "# Define sample texts\n",
    "texts = [\n",
    "    'John loves icecream',\n",
    "    'John loves oranges',\n",
    "    'Marry hates icecream'\n",
    "]\n",
    "\n",
    "# Check corpus size\n",
    "corpus_size = len(texts)\n",
    "print(f'Corpus size: {corpus_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "becbbcca-60c9-43cd-94e3-31e4723ef5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Term Matrix:\n",
      "[0, 1, 1, 1, 0, 0]\n",
      "[0, 0, 1, 1, 0, 1]\n",
      "[1, 1, 0, 0, 1, 0]\n",
      "\n",
      "Unique Terms (Columns): ['hates', 'icecream', 'john', 'loves', 'marry', 'oranges']\n"
     ]
    }
   ],
   "source": [
    "def create_dtm(texts):\n",
    "\n",
    "    if not texts or not all(isinstance(t, str) for t in texts):\n",
    "        raise ValueError(\"Input must be a list of non-empty strings.\")\n",
    "    \n",
    "    # Convert texts to lowercase\n",
    "    texts = [text.lower() for text in texts]\n",
    "    \n",
    "    # Extract unique terms\n",
    "    all_words = [word for text in texts for word in text.split()]\n",
    "    unique_terms = sorted(set(all_words))  # Sorted for consistent column order\n",
    "    \n",
    "    # Create the matrix\n",
    "    dtm = []\n",
    "    for text in texts:\n",
    "        row = [text.split().count(term) for term in unique_terms]\n",
    "        dtm.append(row)\n",
    "    \n",
    "    return dtm, unique_terms\n",
    "\n",
    "dtm, unique_terms = create_dtm(texts)\n",
    "\n",
    "print(\"Document Term Matrix:\")\n",
    "for row in dtm:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nUnique Terms (Columns):\", unique_terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84d1c9d",
   "metadata": {},
   "source": [
    "### Document Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e110fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Term Matrix:\n",
      "[[0 1 1 1 0 0]\n",
      " [0 0 1 1 0 1]\n",
      " [1 1 0 0 1 0]]\n",
      "Terms: ['hates' 'icecream' 'john' 'loves' 'marry' 'oranges']\n"
     ]
    }
   ],
   "source": [
    "# Create a Document Term Matrix using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "dtm = vectorizer.fit_transform(texts).toarray()\n",
    "\n",
    "# Display the matrix and terms\n",
    "print('Document Term Matrix:')\n",
    "print(dtm)\n",
    "print('Terms:', vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7e7aad",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Create a `Document Term Matrix` for the following texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef1a3574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Term Matrix for new texts:\n",
      "[[4 1]\n",
      " [1 3]\n",
      " [2 0]]\n",
      "Terms: ['banana' 'chocolate']\n"
     ]
    }
   ],
   "source": [
    "# Define new sample texts\n",
    "new_texts = [\n",
    "    'banana banana banana banana chocolate',\n",
    "    'chocolate chocolate chocolate banana',\n",
    "    'banana banana'\n",
    "]\n",
    "\n",
    "# Create a Document Term Matrix for new texts\n",
    "new_dtm = vectorizer.fit_transform(new_texts).toarray()\n",
    "\n",
    "print('Document Term Matrix for new texts:')\n",
    "print(new_dtm)\n",
    "print('Terms:', vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c354ad23",
   "metadata": {},
   "source": [
    "## Multinomial Model of Texts\n",
    "The Multinomial Model of Texts is a probabilistic model used to determine the likelihood of a document being generated by a given source based on word probabilities. It is based on the Multinomial Distribution:\n",
    "\n",
    "$$\n",
    "P_n(x_1, x_2, ..., x_n) = \\frac{N!}{x_1! x_2! ... x_n!} p_1^{x_1} \\times p_2^{x_2} \\times ... \\times p_n^{x_n}\n",
    "$$\n",
    "\n",
    "- $N$ is the total number of words in the document.\n",
    "- $x_i$ is the count of the $i$th word.\n",
    "- $p_i$ is the probability of the $i$th word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5574405",
   "metadata": {},
   "source": [
    "### Generating Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b8b96a-ff85-4b28-9830-8fcd91c3c402",
   "metadata": {},
   "source": [
    "Next, we assume that texts 1 through 3 are generated by John, while texts 4 though 6 are generated by Mary. Text_7 and Text_8 are of unknown origin. And we must determine who said what!\n",
    "\n",
    "Create a Document Term matrix and call it banana_data:\n",
    "\n",
    "Next, we need to calculate John and Mary word rates (how often do they use certain words).\n",
    "\n",
    "We do this by summing together all document rates that belong to John and to Mary, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3559717b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Term Matrix:\n",
      "[[4 1 0 0]\n",
      " [2 0 0 0]\n",
      " [1 3 1 0]\n",
      " [0 0 1 3]\n",
      " [0 0 3 0]\n",
      " [0 0 2 2]\n",
      " [0 0 2 1]\n",
      " [2 2 0 0]]\n",
      "Terms: ['banana' 'chocolate' 'fudge' 'icecream']\n"
     ]
    }
   ],
   "source": [
    "# Define a larger corpus\n",
    "banana_texts = [\n",
    "    'banana banana banana banana chocolate',\n",
    "    'banana banana',\n",
    "    'chocolate chocolate chocolate banana fudge',\n",
    "    'icecream icecream fudge icecream',\n",
    "    'fudge fudge fudge',\n",
    "    'icecream icecream fudge fudge',\n",
    "    'icecream fudge fudge',\n",
    "    'chocolate chocolate banana banana'\n",
    "]\n",
    "\n",
    "# Create a Document Term Matrix\n",
    "banana_data = vectorizer.fit_transform(banana_texts).toarray()\n",
    "\n",
    "print('Document Term Matrix:')\n",
    "print(banana_data)\n",
    "print('Terms:', vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb1331b8-3685-4d38-a4ca-7bfad10e7bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 1, 0, 0],\n",
       "       [2, 0, 0, 0],\n",
       "       [1, 3, 1, 0],\n",
       "       [0, 0, 1, 3],\n",
       "       [0, 0, 3, 0],\n",
       "       [0, 0, 2, 2],\n",
       "       [0, 0, 2, 1],\n",
       "       [2, 2, 0, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7a1c969-01ae-4838-a640-7a5e97cd38e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 1, 0, 0],\n",
       "       [2, 0, 0, 0],\n",
       "       [1, 3, 1, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "john_docs = banana_data[0:3, :]\n",
    "\n",
    "john_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfc61839-f6cf-4401-829a-6f5db36bdfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 4, 1, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "john_rates = john_docs.sum(axis=0)\n",
    "\n",
    "john_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83faeea-e539-4a6d-abe6-2f36d2454128",
   "metadata": {},
   "source": [
    "we want to calculate the relative rates (probabilities) of word use. We can do this by dividing the raw rates we have by the total number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78872f5f-0875-4fba-928a-310cf754d524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58333333, 0.33333333, 0.08333333, 0.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_john = john_rates.sum()\n",
    "total_john\n",
    "\n",
    "probs_john = john_rates / total_john\n",
    "probs_john"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b903a-228d-45a9-b3c3-1cdb19d3c6dc",
   "metadata": {},
   "source": [
    "Now, do the same for Mary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c8f5e75-ad54-4096-97b8-0bfbec7a525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "mary_docs = banana_data[3:6, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a99b729e-c914-4a10-8658-ea2b9fdbecc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 3],\n",
       "       [0, 0, 3, 0],\n",
       "       [0, 0, 2, 2]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mary_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78fcc7e0-89d0-4c80-99d7-f358dd79848c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 6, 5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mary_rates = mary_docs.sum(axis=0)\n",
    "mary_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cdad45f-9ae0-47ba-b752-3038b221232f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_mary = mary_rates.sum()\n",
    "total_mary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "019b0cbf-1f2c-4ae5-8d4f-1c2be8ab7b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.54545455, 0.45454545])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_mary = mary_rates / total_mary\n",
    "probs_mary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e00d12-955f-400d-a7a9-cc32ba2f5fc7",
   "metadata": {},
   "source": [
    "## Now we have our Language Models\n",
    "Next, we need to determine the rates of the unknown texts! Remember, the texts are 7 and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d030c886-6ea2-4986-8bb2-5d13dc06f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_1 = banana_data[6, ]\n",
    "unknown_2 = banana_data[7, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aae67f91-88fc-4d2c-9bb5-835a405be68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 1]\n",
      "[2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(unknown_1)\n",
    "\n",
    "print(unknown_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b17d23-753e-4041-9cf3-b478a25d28e4",
   "metadata": {},
   "source": [
    "We can now start calculating!\n",
    "\n",
    "First, let’s determine the probability of both of these texts belonging to John.\n",
    "\n",
    "These are John’s probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45567432-562b-4f5e-adc5-6385c0e291c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58333333, 0.33333333, 0.08333333, 0.        ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_john"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511e8b72",
   "metadata": {},
   "source": [
    "### Example: Calculating Probabilities for Unknown Texts\n",
    "For an unknown text with word counts $[b=0, c=0, f=2, i=1]$, and John's probabilities:\n",
    "\n",
    "$$\n",
    "P(b=0, c=0, f=2, i=1) = \\frac{3!}{0! 0! 2! 1!} \\cdot 0.58^0 \\cdot 0.33^0 \\cdot 0.08^2 \\cdot 0^1\n",
    "$$\n",
    "Step-by-step:\n",
    "\n",
    "1. Calculate the factorial term:\n",
    "\n",
    "$$\n",
    "\\frac{3!}{0! 0! 2! 1!}\n",
    "$$\n",
    "2. Multiply by the word probabilities raised to their respective counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8cdb2ca-3f81-447e-b851-3c9e6821631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_part = factorial(3) / (factorial(0) * factorial(0) * factorial(2) * factorial(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "831200ca-ede0-4301-91de-8ce480ed2d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b352e7a-91a3-424a-be09-e02fd18fb746",
   "metadata": {},
   "source": [
    "Now this: \n",
    "\n",
    "$$\n",
    "0.58^0 \\cdot 0.33^0 \\cdot 0.08^2 \\cdot 0^1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1dfc9f2d-1e0b-4d19-a6a6-40fb22326312",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_part = 0.58**0 * 0.33**0 * 0.08**2 * 0**1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d813b1fa-da28-4b8f-951d-2c01c05c9deb",
   "metadata": {},
   "source": [
    "Combining together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfcae121-5b63-4c19-a0bf-daa126e8fd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_part * second_part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c31dd42-18f4-4546-bc06-c11b10b4229d",
   "metadata": {},
   "source": [
    "This is the probability of the first `unknown` text being generated by john.\n",
    "\n",
    "Now, let's do the second `unknown` text:\n",
    "\n",
    "$$\n",
    "P(b=2, c=2, f=0, i=0) = \\frac{4!}{2!2!0!0!}0.58^{2}\\times 0.33^{2} \\times 0.8^0 \\times 0^0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9bc0f6a-a1a4-4a8e-9c7a-905c1e9f3793",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_part = factorial(4) / (factorial(2) * factorial(2) * factorial(0) * factorial(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e74ac1d9-40f4-411b-9967-9f70d6f7436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise exponentiation and product\n",
    "second_part = np.prod(probs_john ** unknown_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d715c8e-bd7e-4674-989a-1e64db87474a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22685185185185192"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_part * second_part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67204896-ad91-4f37-be10-8ef6b8ea1a78",
   "metadata": {},
   "source": [
    "# Now calculate Mary probabilities yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd09c58",
   "metadata": {},
   "source": [
    "### Example: Calculating Probabilities Using Multinomial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74ce4b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Probabilities: [0.58333333 0.33333333 0.08333333 0.        ]\n",
      "Mary Probabilities: [0.         0.         0.54545455 0.45454545]\n",
      "Unknown Text 1: [0 0 2 1]\n",
      "Unknown Text 2: [2 2 0 0]\n",
      "Probability that Unknown 1 was generated by John: 0.0\n",
      "Probability that Unknown 1 was generated by Mary: 0.4057099924868519\n",
      "Probability that Unknown 2 was generated by John: 0.22685185185185192\n",
      "Probability that Unknown 2 was generated by Mary: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Assuming John generated texts 1-3 and Mary generated texts 4-6\n",
    "john_docs = banana_data[:3, :]\n",
    "mary_docs = banana_data[3:6, :]\n",
    "\n",
    "# Calculate word probabilities for John and Mary\n",
    "john_rates = john_docs.sum(axis=0)\n",
    "mary_rates = mary_docs.sum(axis=0)\n",
    "\n",
    "john_probs = john_rates / john_rates.sum()\n",
    "mary_probs = mary_rates / mary_rates.sum()\n",
    "\n",
    "print('John Probabilities:', john_probs)\n",
    "print('Mary Probabilities:', mary_probs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define unknown texts (rows 7 and 8)\n",
    "unknown_1 = banana_data[6, :]\n",
    "unknown_2 = banana_data[7, :]\n",
    "\n",
    "# Display unknown texts\n",
    "print('Unknown Text 1:', unknown_1)\n",
    "print('Unknown Text 2:', unknown_2)\n",
    "\n",
    "# Calculate probabilities\n",
    "def multinomial_probability(word_counts, word_probs):\n",
    "    N = sum(word_counts)\n",
    "    numerator = factorial(N)\n",
    "    denominator = np.prod([factorial(c) for c in word_counts])\n",
    "    probs = np.prod(word_probs ** word_counts)\n",
    "    return (numerator / denominator) * probs\n",
    "\n",
    "# Example for Unknown Text 1\n",
    "p_john_unknown_1 = multinomial_probability(unknown_1, john_probs)\n",
    "p_john_unknown_2 = multinomial_probability(unknown_2, john_probs)\n",
    "\n",
    "# Similarly, calculate for Mary\n",
    "p_mary_unknown_1 = multinomial_probability(unknown_1, mary_probs)\n",
    "p_mary_unknown_2 = multinomial_probability(unknown_2, mary_probs)\n",
    "\n",
    "print(f'Probability that Unknown 1 was generated by John: {p_john_unknown_1}')\n",
    "print(f'Probability that Unknown 1 was generated by Mary: {p_mary_unknown_1}')\n",
    "\n",
    "print(f'Probability that Unknown 2 was generated by John: {p_john_unknown_2}')\n",
    "print(f'Probability that Unknown 2 was generated by Mary: {p_mary_unknown_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e2b01b",
   "metadata": {},
   "source": [
    "### Predicting Text Origin with the Multinomial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea1900f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b99c1daa",
   "metadata": {},
   "source": [
    "### Task: Calculate Probabilities for Mary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc3652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same process to calculate probabilities for Mary.\n",
    "# Define unknown texts (rows 7 and 8) and use Mary probabilities.\n",
    "# Mary probabilities are already calculated as `mary_probs`. Proceed as done for John.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
